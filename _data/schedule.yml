- Title: Opening Remarks
  Time: 8:15AM - 8:30AM
  Whole_line: true  

- Title: <b>KeyNote Talk </b>
  Time: 8:30AM - 9:00AM
  Presenter: Prof. Azalia Mirhoseini
  Is_paper: false
  Whole_line: false
  Bio: TBD
  Abstract: TBD

- Title: <b>KeyNote Talk</b>
  Time: 9:00AM - 9:30AM
  Presenter: Dr. Weizhu Chen
  Is_paper: false
  Whole_line: false
  Bio: TBD
  Abstract: TBD

- Title: <b>Accepted Oral Presentations - Part I</b>
  Time: 9:30AM - 10:00AM
  Whole_line: false

- Title: <b>Morning Networking Break</b>
  Time: 10:00AM - 10:30AM
  Whole_line: true  


- Title: <b>KeyNote Talk</b>
  Time: 10:30AM - 11:00AM
  Presenter: Dr. Peter Clark
  Is_paper: false
  Whole_line: false
  Bio: TBD
  Abstract: TBD

- Title: <b>KeyNote Talk</b>
  Time: 11:30AM - 11:30AM
  Presenter: Prof. Barbara Plank
  Is_paper: false
  Whole_line: false
  Bio: TBD
  Abstract: TBD
- Title: (<b>Accepted Oral Presentations - Part II</b>)
  Time: 11:30AM - 12:00AM
  Whole_line: true

- Title: (<b>Selected Special Demo Track Presentation</b>)
  Time: 12:00 - 12:15 PM
  Whole_line: true

- Title: <b>Sponsored Lunch & Networking Break</b>
  Time: 12:15PM - 1:00PM
  Whole_line: true
- Title: <b>Poster Session I-(Paper IDs &#35;1 - &#35;50 <a href="https://drive.google.com/drive/folders/1xnuRixS8p8LvBOzj2rc5B8R2QdVAez82?usp=sharing" target="_blank">[Link to Posters]</a>) </b>
  Time: 12:30PM - 1:30PM
  Whole_line: true  
- Title: (<b>Spotlight 6</b>) RetrievalAttention&#58; Accelerating Long-Context LLM Inference via Vector Retrieval
  Time: 1:30PM - 1:36PM
  Presenter: Huiqiang Jiang
  Is_paper: true
  Whole_line: false
  Authors: Di Liu (Shanghai Jiao Tong University),Meng Chen (Fudan University),Baotong Lu (Microsoft Research)*,Huiqiang Jiang (Microsoft Research Asia),Zhenhua Han (Microsoft),Qianxi Zhang (MSRA),Qi Chen (Microsoft Research Asia),Chengruidong Zhang (MSFT),Bailu Ding (Microsoft Research),Kai Zhang (Fudan University),Chen Chen (Shanghai Jiao Tong University),Fan Yang (MSRA),Yuqing Yang (Microsoft),Lili Qiu (Microsoft Research Asia
  Abstract: Transformer-based Large Language Models (LLMs) have become increasingly important. However, due to the quadratic time complexity of attention computation, scaling LLMs to longer contexts incurs extremely slow inference latency and high GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to both accelerate attention computation and reduce GPU memory consumption. By leveraging the dynamic sparsity of attention mechanism, RetrievalAttention proposes to use approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory and retrieves the most relevant ones with vector search during generation. Unfortunately, we observe that the off-the-shelf ANNS indexes are often ineffective for such retrieval tasks due to the out-of-distribution (OOD) between query vectors and key vectors in attention mechanism. RetrievalAttention addresses the OOD challenge by designing an attention-aware vector search algorithm that can adapt to the distribution of query vectors. Our evaluation shows that RetrievalAttention only needs to access 1--3% of data while maintaining high model accuracy. This leads to significant reduction in the inference cost of long-context LLMs with much lower GPU memory footprint. In particular, RetrievalAttention only needs a single NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds.
- Title: (<b>Spotlight 7</b>) Post-Training Statistical Calibration for Higher Activation Sparsity
  Time: 1:36PM - 1:42PM
  Presenter: Vui Seng Chua
  Is_paper: true
  Whole_line: false
  Authors: Vui Seng Chua (Intel Corporation),Yujie Pan (Intel)*,Nilesh Jain (Intel)
  Abstract: We present Statistical Calibrated Activation Pruning (SCAP), a post-training activation pruning framework that (1) generalizes sparsification by input activations of Fully-Connected layers for generic and flexible application across Transformers, and (2) features a simple Mode-Centering technique to pre-calibrate activation distributions for maximizing post-training sparsity. Our results demonstrate robust Pareto efficiency compared to prior methods, translating to a 1.5× additional LLM decoding speedup against CATS[12] at iso model quality. SCAP effectiveness is empirically verified across a wide range of models, including recent Transformer Decoders, MoE, Mamba2, Encoding Transformer, and pre-quantized models, highlighting its practicality and scalability. The code is available at https://github.com/IntelLabs/SCAP.
- Title: (<b>Spotlight 8</b>) Bio-xLSTM&#58; Generative modeling, representation and in-context learning of biological and chemical sequences
  Time: 1:42PM - 1:48PM
  Presenter: Niklas Schmidinger
  Is_paper: true
  Whole_line: false
  Authors: Niklas Schmidinger (Johannes Kepler University Linz)*,Lisa Schneckenreiter (Johannes Kepler University, Linz),Philipp Seidl (JKU Linz),Johannes Schimunek (Johannes Kepler University Linz),Pieter-Jan Hoedt (LIT AI Lab, Institute for Machine Learning, Johannes Kepler University Linz, Austria),Johannes Brandstetter (LIT AI Lab, Institute for Machine Learning, Johannes Kepler University Linz, Austria),Andreas Mayr (Johannes Kepler University Linz),Sohvi Luukkonen (Johannes Kepler University),Sepp Hochreiter (LIT AI Lab, Institute for Machine Learning, Johannes Kepler University, NXAI GmbH, Linz, Austria),Guenter Klambauer (LIT AI Lab)
  Abstract: Language models for biological and chemical sequences enable crucial applications such as drug discovery, protein engineering, and precision medicine. Currently, these language models are predominantly based on Transformer architectures. While Transformers have yielded impressive results, their quadratic runtime dependency on sequence length complicates their use for long genomic sequences and in-context learning on proteins and chemical sequences. Recently, the recurrent xLSTM architecture has been shown to perform favorably compared to Transformers and modern state-space models (SSMs) in the natural language domain. Similar to SSMs, xLSTMs have linear runtime dependency and allow for constant-memory decoding at inference time, which makes them prime candidates for modeling long-range dependencies in biological and chemical sequences. In this work, we tailor xLSTM towards these domains and we propose a suite of language models called Bio-xLSTM. Extensive experiments in three large domains, genomics, proteins, and chemistry, were performed to assess xLSTM's ability to model biological and chemical sequences. The results show that Bio-xLSTM is a highly proficient generative model for DNA, protein, and chemical sequences, learns rich representations, and can perform in-context learning for proteins and small molecules.
- Title: (<b>Spotlight 9</b>) Inference-Friendly Models With MixAttention
  Time: 1:48PM - 1:54PM
  Presenter: Shashank Rajput
  Is_paper: true
  Whole_line: false
  Authors: Shashank Rajput (Databricks)*,Ying Sheng (NA),Sean Owen (Databricks),Vitaliy Chiley (Cerebras)
  Abstract: The size of the key-value (KV) cache plays a critical role in determining both the maximum context length and the number of concurrent requests supported during inference in modern language models. The KV cache size grows proportionally with the number of attention heads and the tokens processed, leading to increased memory consumption and slower inference for long inputs. In this work, we explore the use of MixAttention, a model architecture modification closely related to a blog published by Character.AI. MixAttention combines sliding window attention, where only a small subset of recent tokens is stored in the KV cache, with KV cache sharing across layers. Our experiments demonstrate that MixAttention significantly reduces memory usage and improves inference speed without sacrificing model performance in both short and long-context tasks. We also explore various configurations of this architecture, identifying those that maintain quality across evaluation metrics while optimizing resource efficiency.
- Title: (<b>Spotlight 10</b>) One Initialization to Rule them All&#58; Fine-tuning via Explained Variance Adaptation
  Time: 1:54PM - 2:00PM
  Presenter: Fabian Paischer
  Is_paper: true
  Whole_line: false
  Authors: Fabian Paischer (ELLIS Unit Linz, LIT AI Lab, Institute for Machine Learning, Johannes Kepler University Linz, Austria)*,Lukas Hauzenberger (ELLIS Unit Linz, LIT AI Lab, Institute for Machine Learning, Johannes Kepler University Linz),Thomas Schmied (ELLIS Unit Linz, LIT AI Lab, Institute for Machine Learning, Johannes Kepler University Linz),Benedikt Alkin (Institute for Machine Learning),Marc Deisenroth (University College London),Sepp Hochreiter (LIT AI Lab, In
  Abstract: Abstract TBD
- Title: (<b>KeyNote Talk</b>)  The LoRA Journey and Learnings&#58; from Creation to Industrial-Scale Adoption
  Time: 2:00PM - 2:30PM
  Presenter: Dr. Weizhu Chen
  Is_paper: false
  Whole_line: false
  Bio: <b>Weizhu Chen</b> Weizhu Chen is the Vice President leading the Microsoft GenAI modeling team, driving innovation in large-scale AI model training, including pre-training, post-training, and evaluation for both Microsoft and OpenAI. Under his leadership, the team has pioneered groundbreaking advancements such as LoRA, DeBERTa, and Phi-3 models. With over 19 years at Microsoft, Weizhu has held pivotal roles in shaping AI and machine learning technologies. Previously, he served as Partner Science Manager at Microsoft Azure AI and led teams in the Business Applications Group and Research divisions, focusing on deep learning, NLP, and distributed machine learning at cloud scale. Before joining Microsoft, he contributed to research on information retrieval at IBM Research. Weizhu’s career reflects a deep commitment to advancing the state of AI, making a significant impact on the field and enabling transformative technologies.
  Abstract: Abstract TBD
  
- Title: (<b>KeyNote Talk</b>) How to build fully open language models&#58; from pre-training to post-training
  Time: 2:30PM - 3:00PM
  Presenter: Prof. Hananeh Hajishirzi
  Is_paper: false
  Whole_line: false
  Bio: <b>Hananeh Hajishirzi</b>, Hanna Hajishirzi is the Torode Family Associate Professor in the Allen School of Computer Science and Engineering at the University of Washington and a Senior Director of NLP at AI2. Her current research delves into various domains within Natural Language Processing (NLP) and Artificial Intelligence (AI), with a particular emphasis on accelerating the science of language modeling, broadening their scope, and enhancing their applicability and usefulness for human lives. She has published over 140 scientific articles in prestigious journals and conferences across ML, AI, NLP, and Computer Vision. She is the recipient of numerous awards, including the Sloan Fellowship, NSF CAREER Award, Intel Rising Star Award, Allen Distinguished Investigator Award, Academic Achievement UIUC Alumni Award, and Innovator of the Year Award by GeekWire. The work from her lab has been nominated for or has received best paper awards at various conferences and has been featured in numerous magazines and newspapers.
  Abstract: Language models (LMs) have become ubiquitous in both AI research and commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. In this talk, I present our OLMo project aimed at building strong language models and making them fully accessible to researchers along with open-source code for data, training, and inference. Training language models are expensive, therefore we optimize for quality vs. compute cost. I focus on how data, architecture, and training improvements advance models at pre-training and post-training stages with less compute cost.




- Title: Afternoon Break
  Time: 3:00PM - 3:30PM
  Whole_line: true

- Title: <b>Interactive Panel Discussion</b>
  Time: 3:30PM - 4:20PM
  Presenter: <ul><li> Marjan Ghazvini Nejad</li><li> Joel Hestness</li><li>Navdeep Jaitly</li><li> Katie Derthick</li></ul>
  Is_paper: false
  Whole_line: false
  
- Title: Best Paper Awards and Closing Remarks 
  Time: 4:20PM-4:30PM
  Whole_line: true  
  
- Title: <b>Poster Session II-(Paper IDs &#35;51 - &#35;105 <a href="https://drive.google.com/drive/folders/1CvigbdoFB95l5zeRYJsyVWhdUuRIvpUs?usp=sharing" target="_blank">[Link to Posters]</a>) </b>
  Time: 4:30PM - 5:30PM
  Whole_line: true



